{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, img_path, json_path):\n",
    "        self.img_path = img_path\n",
    "        with open(json_path, \"r\") as fh:\n",
    "            self.data = json.load(fh)\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        img = cv2.imread(f\"{self.img_path}/{item['id']}.png\")\n",
    "        h,w = img.shape[:2]\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        kps = np.array(item[\"kps\"]).flatten()\n",
    "        kps = kps.astype(np.float32)\n",
    "\n",
    "        kps[::2] *= 224.0 / w\n",
    "        kps[1::2] *= 224.0 / h\n",
    "\n",
    "        return img, kps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = KeypointDataset(\"tennis_court_det_dataset/data/images\", \"tennis_court_det_dataset/data/data_train.json\")\n",
    "val_dataset = KeypointDataset(\"tennis_court_det_dataset/data/images\", \"tennis_court_det_dataset/data/data_val.json\")\n",
    "\n",
    "train_loader = DataLoader(training_dataset, batch_size = 16, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jurko\\anaconda3\\envs\\tennis_analyzer\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jurko\\anaconda3\\envs\\tennis_analyzer\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 14*2) # Replace the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=28, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iter 0, loss: 14937.2294921875\n",
      "Epoch 0, iter 10, loss: 14701.458984375\n",
      "Epoch 0, iter 20, loss: 14067.0869140625\n",
      "Epoch 0, iter 30, loss: 13720.052734375\n",
      "Epoch 0, iter 40, loss: 13659.21875\n",
      "Epoch 0, iter 50, loss: 13226.248046875\n",
      "Epoch 0, iter 60, loss: 12777.7939453125\n",
      "Epoch 0, iter 70, loss: 12242.228515625\n",
      "Epoch 0, iter 80, loss: 11415.677734375\n",
      "Epoch 0, iter 90, loss: 11058.9521484375\n",
      "Epoch 0, iter 100, loss: 10978.4599609375\n",
      "Epoch 0, iter 110, loss: 10592.1826171875\n",
      "Epoch 0, iter 120, loss: 10305.845703125\n",
      "Epoch 0, iter 130, loss: 10293.701171875\n",
      "Epoch 0, iter 140, loss: 9873.791015625\n",
      "Epoch 0, iter 150, loss: 9748.3740234375\n",
      "Epoch 0, iter 160, loss: 8943.244140625\n",
      "Epoch 0, iter 170, loss: 9006.0107421875\n",
      "Epoch 0, iter 180, loss: 8504.189453125\n",
      "Epoch 0, iter 190, loss: 8013.3134765625\n",
      "Epoch 0, iter 200, loss: 8289.111328125\n",
      "Epoch 0, iter 210, loss: 7444.2880859375\n",
      "Epoch 0, iter 220, loss: 7221.99609375\n",
      "Epoch 0, iter 230, loss: 7140.18408203125\n",
      "Epoch 0, iter 240, loss: 6603.02197265625\n",
      "Epoch 0, iter 250, loss: 6118.8583984375\n",
      "Epoch 0, iter 260, loss: 6059.21533203125\n",
      "Epoch 0, iter 270, loss: 5840.7587890625\n",
      "Epoch 0, iter 280, loss: 5866.09521484375\n",
      "Epoch 0, iter 290, loss: 5234.05322265625\n",
      "Epoch 0, iter 300, loss: 5094.61865234375\n",
      "Epoch 0, iter 310, loss: 4960.3955078125\n",
      "Epoch 0, iter 320, loss: 4662.07177734375\n",
      "Epoch 0, iter 330, loss: 4386.37060546875\n",
      "Epoch 0, iter 340, loss: 4575.38037109375\n",
      "Epoch 0, iter 350, loss: 4509.93896484375\n",
      "Epoch 0, iter 360, loss: 3930.684814453125\n",
      "Epoch 0, iter 370, loss: 3877.253173828125\n",
      "Epoch 0, iter 380, loss: 3994.064697265625\n",
      "Epoch 0, iter 390, loss: 3616.12548828125\n",
      "Epoch 0, iter 400, loss: 3246.60498046875\n",
      "Epoch 0, iter 410, loss: 3249.152099609375\n",
      "Epoch 1, iter 0, loss: 3008.708251953125\n",
      "Epoch 1, iter 10, loss: 2805.38671875\n",
      "Epoch 1, iter 20, loss: 2746.024658203125\n",
      "Epoch 1, iter 30, loss: 2799.67529296875\n",
      "Epoch 1, iter 40, loss: 2504.155517578125\n",
      "Epoch 1, iter 50, loss: 2475.677001953125\n",
      "Epoch 1, iter 60, loss: 2404.154052734375\n",
      "Epoch 1, iter 70, loss: 2088.96142578125\n",
      "Epoch 1, iter 80, loss: 1944.3734130859375\n",
      "Epoch 1, iter 90, loss: 1948.675537109375\n",
      "Epoch 1, iter 100, loss: 1849.2237548828125\n",
      "Epoch 1, iter 110, loss: 1775.069580078125\n",
      "Epoch 1, iter 120, loss: 1908.1759033203125\n",
      "Epoch 1, iter 130, loss: 1919.039306640625\n",
      "Epoch 1, iter 140, loss: 1339.0931396484375\n",
      "Epoch 1, iter 150, loss: 1438.760009765625\n",
      "Epoch 1, iter 160, loss: 1254.378662109375\n",
      "Epoch 1, iter 170, loss: 1508.1676025390625\n",
      "Epoch 1, iter 180, loss: 1311.8016357421875\n",
      "Epoch 1, iter 190, loss: 1245.5445556640625\n",
      "Epoch 1, iter 200, loss: 984.4044189453125\n",
      "Epoch 1, iter 210, loss: 926.8401489257812\n",
      "Epoch 1, iter 220, loss: 1019.7631225585938\n",
      "Epoch 1, iter 230, loss: 864.3009033203125\n",
      "Epoch 1, iter 240, loss: 823.8342895507812\n",
      "Epoch 1, iter 250, loss: 655.1148681640625\n",
      "Epoch 1, iter 260, loss: 736.30859375\n",
      "Epoch 1, iter 270, loss: 631.5927124023438\n",
      "Epoch 1, iter 280, loss: 779.6909790039062\n",
      "Epoch 1, iter 290, loss: 581.2181396484375\n",
      "Epoch 1, iter 300, loss: 441.6204528808594\n",
      "Epoch 1, iter 310, loss: 459.3968200683594\n",
      "Epoch 1, iter 320, loss: 485.2164611816406\n",
      "Epoch 1, iter 330, loss: 489.71923828125\n",
      "Epoch 1, iter 340, loss: 434.11517333984375\n",
      "Epoch 1, iter 350, loss: 335.80621337890625\n",
      "Epoch 1, iter 360, loss: 449.900390625\n",
      "Epoch 1, iter 370, loss: 388.22119140625\n",
      "Epoch 1, iter 380, loss: 367.2569274902344\n",
      "Epoch 1, iter 390, loss: 266.78192138671875\n",
      "Epoch 1, iter 400, loss: 261.234130859375\n",
      "Epoch 1, iter 410, loss: 316.88470458984375\n",
      "Epoch 2, iter 0, loss: 238.704833984375\n",
      "Epoch 2, iter 10, loss: 190.5157470703125\n",
      "Epoch 2, iter 20, loss: 210.05113220214844\n",
      "Epoch 2, iter 30, loss: 214.0689239501953\n",
      "Epoch 2, iter 40, loss: 161.82962036132812\n",
      "Epoch 2, iter 50, loss: 257.18719482421875\n",
      "Epoch 2, iter 60, loss: 164.93380737304688\n",
      "Epoch 2, iter 70, loss: 206.0259246826172\n",
      "Epoch 2, iter 80, loss: 148.2612762451172\n",
      "Epoch 2, iter 90, loss: 153.53555297851562\n",
      "Epoch 2, iter 100, loss: 149.84732055664062\n",
      "Epoch 2, iter 110, loss: 107.94658660888672\n",
      "Epoch 2, iter 120, loss: 111.92810821533203\n",
      "Epoch 2, iter 130, loss: 85.84821319580078\n",
      "Epoch 2, iter 140, loss: 93.87311553955078\n",
      "Epoch 2, iter 150, loss: 113.06261444091797\n",
      "Epoch 2, iter 160, loss: 123.48060607910156\n",
      "Epoch 2, iter 170, loss: 194.43861389160156\n",
      "Epoch 2, iter 180, loss: 77.38115692138672\n",
      "Epoch 2, iter 190, loss: 71.60792541503906\n",
      "Epoch 2, iter 200, loss: 137.01478576660156\n",
      "Epoch 2, iter 210, loss: 67.51789855957031\n",
      "Epoch 2, iter 220, loss: 74.35786437988281\n",
      "Epoch 2, iter 230, loss: 43.7188720703125\n",
      "Epoch 2, iter 240, loss: 70.41371154785156\n",
      "Epoch 2, iter 250, loss: 62.24252700805664\n",
      "Epoch 2, iter 260, loss: 118.80977630615234\n",
      "Epoch 2, iter 270, loss: 52.332542419433594\n",
      "Epoch 2, iter 280, loss: 98.9270248413086\n",
      "Epoch 2, iter 290, loss: 71.41743469238281\n",
      "Epoch 2, iter 300, loss: 55.12515640258789\n",
      "Epoch 2, iter 310, loss: 51.01127243041992\n",
      "Epoch 2, iter 320, loss: 78.80094909667969\n",
      "Epoch 2, iter 330, loss: 109.76912689208984\n",
      "Epoch 2, iter 340, loss: 86.19573211669922\n",
      "Epoch 2, iter 350, loss: 71.33491516113281\n",
      "Epoch 2, iter 360, loss: 51.255615234375\n",
      "Epoch 2, iter 370, loss: 62.591941833496094\n",
      "Epoch 2, iter 380, loss: 49.038944244384766\n",
      "Epoch 2, iter 390, loss: 60.324554443359375\n",
      "Epoch 2, iter 400, loss: 23.470672607421875\n",
      "Epoch 2, iter 410, loss: 35.4750862121582\n",
      "Epoch 3, iter 0, loss: 62.69941329956055\n",
      "Epoch 3, iter 10, loss: 54.2070426940918\n",
      "Epoch 3, iter 20, loss: 32.00310134887695\n",
      "Epoch 3, iter 30, loss: 32.200260162353516\n",
      "Epoch 3, iter 40, loss: 79.26532745361328\n",
      "Epoch 3, iter 50, loss: 45.01020050048828\n",
      "Epoch 3, iter 60, loss: 32.39604949951172\n",
      "Epoch 3, iter 70, loss: 51.358150482177734\n",
      "Epoch 3, iter 80, loss: 57.40532302856445\n",
      "Epoch 3, iter 90, loss: 50.45183181762695\n",
      "Epoch 3, iter 100, loss: 15.287322998046875\n",
      "Epoch 3, iter 110, loss: 39.593875885009766\n",
      "Epoch 3, iter 120, loss: 51.450225830078125\n",
      "Epoch 3, iter 130, loss: 46.324180603027344\n",
      "Epoch 3, iter 140, loss: 84.51438903808594\n",
      "Epoch 3, iter 150, loss: 25.356199264526367\n",
      "Epoch 3, iter 160, loss: 29.637493133544922\n",
      "Epoch 3, iter 170, loss: 29.285858154296875\n",
      "Epoch 3, iter 180, loss: 107.73638916015625\n",
      "Epoch 3, iter 190, loss: 36.76702117919922\n",
      "Epoch 3, iter 200, loss: 33.364723205566406\n",
      "Epoch 3, iter 210, loss: 41.536861419677734\n",
      "Epoch 3, iter 220, loss: 42.4648323059082\n",
      "Epoch 3, iter 230, loss: 51.515464782714844\n",
      "Epoch 3, iter 240, loss: 25.247751235961914\n",
      "Epoch 3, iter 250, loss: 72.34879302978516\n",
      "Epoch 3, iter 260, loss: 29.912752151489258\n",
      "Epoch 3, iter 270, loss: 40.25979232788086\n",
      "Epoch 3, iter 280, loss: 91.52579498291016\n",
      "Epoch 3, iter 290, loss: 37.994346618652344\n",
      "Epoch 3, iter 300, loss: 30.81075096130371\n",
      "Epoch 3, iter 310, loss: 71.58340454101562\n",
      "Epoch 3, iter 320, loss: 38.33693313598633\n",
      "Epoch 3, iter 330, loss: 243.12338256835938\n",
      "Epoch 3, iter 340, loss: 24.16697883605957\n",
      "Epoch 3, iter 350, loss: 43.67316436767578\n",
      "Epoch 3, iter 360, loss: 151.32603454589844\n",
      "Epoch 3, iter 370, loss: 26.253080368041992\n",
      "Epoch 3, iter 380, loss: 34.434226989746094\n",
      "Epoch 3, iter 390, loss: 70.38838958740234\n",
      "Epoch 3, iter 400, loss: 29.778085708618164\n",
      "Epoch 3, iter 410, loss: 19.48324203491211\n",
      "Epoch 4, iter 0, loss: 43.529117584228516\n",
      "Epoch 4, iter 10, loss: 25.5789794921875\n",
      "Epoch 4, iter 20, loss: 30.989883422851562\n",
      "Epoch 4, iter 30, loss: 37.377899169921875\n",
      "Epoch 4, iter 40, loss: 101.77709197998047\n",
      "Epoch 4, iter 50, loss: 21.534624099731445\n",
      "Epoch 4, iter 60, loss: 27.800373077392578\n",
      "Epoch 4, iter 70, loss: 66.46784973144531\n",
      "Epoch 4, iter 80, loss: 171.61428833007812\n",
      "Epoch 4, iter 90, loss: 35.03919219970703\n",
      "Epoch 4, iter 100, loss: 71.72291564941406\n",
      "Epoch 4, iter 110, loss: 42.047786712646484\n",
      "Epoch 4, iter 120, loss: 29.659025192260742\n",
      "Epoch 4, iter 130, loss: 64.03691101074219\n",
      "Epoch 4, iter 140, loss: 26.02964973449707\n",
      "Epoch 4, iter 150, loss: 21.72556495666504\n",
      "Epoch 4, iter 160, loss: 21.401081085205078\n",
      "Epoch 4, iter 170, loss: 50.64091873168945\n",
      "Epoch 4, iter 180, loss: 33.753055572509766\n",
      "Epoch 4, iter 190, loss: 21.721166610717773\n",
      "Epoch 4, iter 200, loss: 27.06999397277832\n",
      "Epoch 4, iter 210, loss: 17.93069839477539\n",
      "Epoch 4, iter 220, loss: 36.120765686035156\n",
      "Epoch 4, iter 230, loss: 31.22869300842285\n",
      "Epoch 4, iter 240, loss: 24.5471134185791\n",
      "Epoch 4, iter 250, loss: 33.40034103393555\n",
      "Epoch 4, iter 260, loss: 30.04336929321289\n",
      "Epoch 4, iter 270, loss: 18.605947494506836\n",
      "Epoch 4, iter 280, loss: 20.24350929260254\n",
      "Epoch 4, iter 290, loss: 78.12950897216797\n",
      "Epoch 4, iter 300, loss: 17.36492347717285\n",
      "Epoch 4, iter 310, loss: 43.386775970458984\n",
      "Epoch 4, iter 320, loss: 15.681650161743164\n",
      "Epoch 4, iter 330, loss: 17.64570426940918\n",
      "Epoch 4, iter 340, loss: 24.630346298217773\n",
      "Epoch 4, iter 350, loss: 33.33835983276367\n",
      "Epoch 4, iter 360, loss: 36.966064453125\n",
      "Epoch 4, iter 370, loss: 16.97699546813965\n",
      "Epoch 4, iter 380, loss: 37.75300216674805\n",
      "Epoch 4, iter 390, loss: 38.721343994140625\n",
      "Epoch 4, iter 400, loss: 21.412057876586914\n",
      "Epoch 4, iter 410, loss: 23.68398666381836\n",
      "Epoch 5, iter 0, loss: 18.42300033569336\n",
      "Epoch 5, iter 10, loss: 43.077125549316406\n",
      "Epoch 5, iter 20, loss: 33.01761245727539\n",
      "Epoch 5, iter 30, loss: 9.190786361694336\n",
      "Epoch 5, iter 40, loss: 46.075775146484375\n",
      "Epoch 5, iter 50, loss: 14.707503318786621\n",
      "Epoch 5, iter 60, loss: 26.50701332092285\n",
      "Epoch 5, iter 70, loss: 15.358719825744629\n",
      "Epoch 5, iter 80, loss: 30.122203826904297\n",
      "Epoch 5, iter 90, loss: 34.21613311767578\n",
      "Epoch 5, iter 100, loss: 21.557226181030273\n",
      "Epoch 5, iter 110, loss: 16.48731803894043\n",
      "Epoch 5, iter 120, loss: 17.273427963256836\n",
      "Epoch 5, iter 130, loss: 115.47560119628906\n",
      "Epoch 5, iter 140, loss: 19.926513671875\n",
      "Epoch 5, iter 150, loss: 16.970592498779297\n",
      "Epoch 5, iter 160, loss: 64.86421966552734\n",
      "Epoch 5, iter 170, loss: 13.670495986938477\n",
      "Epoch 5, iter 180, loss: 31.8817195892334\n",
      "Epoch 5, iter 190, loss: 15.082453727722168\n",
      "Epoch 5, iter 200, loss: 17.345165252685547\n",
      "Epoch 5, iter 210, loss: 25.6189022064209\n",
      "Epoch 5, iter 220, loss: 98.24190521240234\n",
      "Epoch 5, iter 230, loss: 29.532487869262695\n",
      "Epoch 5, iter 240, loss: 11.516968727111816\n",
      "Epoch 5, iter 250, loss: 21.474092483520508\n",
      "Epoch 5, iter 260, loss: 27.9224796295166\n",
      "Epoch 5, iter 270, loss: 70.45809173583984\n",
      "Epoch 5, iter 280, loss: 42.04133224487305\n",
      "Epoch 5, iter 290, loss: 18.560949325561523\n",
      "Epoch 5, iter 300, loss: 83.02936553955078\n",
      "Epoch 5, iter 310, loss: 14.428472518920898\n",
      "Epoch 5, iter 320, loss: 31.591110229492188\n",
      "Epoch 5, iter 330, loss: 80.09791564941406\n",
      "Epoch 5, iter 340, loss: 31.90511131286621\n",
      "Epoch 5, iter 350, loss: 14.74381160736084\n",
      "Epoch 5, iter 360, loss: 21.815174102783203\n",
      "Epoch 5, iter 370, loss: 21.695526123046875\n",
      "Epoch 5, iter 380, loss: 16.417287826538086\n",
      "Epoch 5, iter 390, loss: 22.0617618560791\n",
      "Epoch 5, iter 400, loss: 15.370251655578613\n",
      "Epoch 5, iter 410, loss: 9.781904220581055\n",
      "Epoch 6, iter 0, loss: 17.110265731811523\n",
      "Epoch 6, iter 10, loss: 19.462377548217773\n",
      "Epoch 6, iter 20, loss: 21.275026321411133\n",
      "Epoch 6, iter 30, loss: 43.020206451416016\n",
      "Epoch 6, iter 40, loss: 52.383079528808594\n",
      "Epoch 6, iter 50, loss: 58.626304626464844\n",
      "Epoch 6, iter 60, loss: 16.276092529296875\n",
      "Epoch 6, iter 70, loss: 16.507434844970703\n",
      "Epoch 6, iter 80, loss: 14.426936149597168\n",
      "Epoch 6, iter 90, loss: 27.525686264038086\n",
      "Epoch 6, iter 100, loss: 13.55675220489502\n",
      "Epoch 6, iter 110, loss: 29.42659568786621\n",
      "Epoch 6, iter 120, loss: 11.826871871948242\n",
      "Epoch 6, iter 130, loss: 9.587117195129395\n",
      "Epoch 6, iter 140, loss: 20.457561492919922\n",
      "Epoch 6, iter 150, loss: 15.859990119934082\n",
      "Epoch 6, iter 160, loss: 85.3856201171875\n",
      "Epoch 6, iter 170, loss: 24.791776657104492\n",
      "Epoch 6, iter 180, loss: 14.884124755859375\n",
      "Epoch 6, iter 190, loss: 19.16672706604004\n",
      "Epoch 6, iter 200, loss: 35.2664794921875\n",
      "Epoch 6, iter 210, loss: 31.6741886138916\n",
      "Epoch 6, iter 220, loss: 10.955574989318848\n",
      "Epoch 6, iter 230, loss: 13.120485305786133\n",
      "Epoch 6, iter 240, loss: 15.07283878326416\n",
      "Epoch 6, iter 250, loss: 15.737472534179688\n",
      "Epoch 6, iter 260, loss: 13.809988021850586\n",
      "Epoch 6, iter 270, loss: 19.58656883239746\n",
      "Epoch 6, iter 280, loss: 14.050082206726074\n",
      "Epoch 6, iter 290, loss: 16.047372817993164\n",
      "Epoch 6, iter 300, loss: 8.6458740234375\n",
      "Epoch 6, iter 310, loss: 10.526288986206055\n",
      "Epoch 6, iter 320, loss: 14.459197998046875\n",
      "Epoch 6, iter 330, loss: 7.777931213378906\n",
      "Epoch 6, iter 340, loss: 17.82529640197754\n",
      "Epoch 6, iter 350, loss: 13.137621879577637\n",
      "Epoch 6, iter 360, loss: 16.62430763244629\n",
      "Epoch 6, iter 370, loss: 22.990095138549805\n",
      "Epoch 6, iter 380, loss: 23.35104751586914\n",
      "Epoch 6, iter 390, loss: 14.116854667663574\n",
      "Epoch 6, iter 400, loss: 15.683523178100586\n",
      "Epoch 6, iter 410, loss: 28.046579360961914\n",
      "Epoch 7, iter 0, loss: 59.06864547729492\n",
      "Epoch 7, iter 10, loss: 9.267067909240723\n",
      "Epoch 7, iter 20, loss: 23.097055435180664\n",
      "Epoch 7, iter 30, loss: 7.322287559509277\n",
      "Epoch 7, iter 40, loss: 12.710806846618652\n",
      "Epoch 7, iter 50, loss: 8.272860527038574\n",
      "Epoch 7, iter 60, loss: 7.292534351348877\n",
      "Epoch 7, iter 70, loss: 28.06715202331543\n",
      "Epoch 7, iter 80, loss: 10.449383735656738\n",
      "Epoch 7, iter 90, loss: 8.966532707214355\n",
      "Epoch 7, iter 100, loss: 4.533411026000977\n",
      "Epoch 7, iter 110, loss: 17.97943115234375\n",
      "Epoch 7, iter 120, loss: 63.17422866821289\n",
      "Epoch 7, iter 130, loss: 9.16069507598877\n",
      "Epoch 7, iter 140, loss: 7.64223051071167\n",
      "Epoch 7, iter 150, loss: 15.158357620239258\n",
      "Epoch 7, iter 160, loss: 3.5889153480529785\n",
      "Epoch 7, iter 170, loss: 8.071882247924805\n",
      "Epoch 7, iter 180, loss: 6.4538726806640625\n",
      "Epoch 7, iter 190, loss: 17.91849136352539\n",
      "Epoch 7, iter 200, loss: 12.96789836883545\n",
      "Epoch 7, iter 210, loss: 10.15418529510498\n",
      "Epoch 7, iter 220, loss: 9.869460105895996\n",
      "Epoch 7, iter 230, loss: 4.26413631439209\n",
      "Epoch 7, iter 240, loss: 12.854926109313965\n",
      "Epoch 7, iter 250, loss: 11.684829711914062\n",
      "Epoch 7, iter 260, loss: 5.8355584144592285\n",
      "Epoch 7, iter 270, loss: 184.9075927734375\n",
      "Epoch 7, iter 280, loss: 8.975868225097656\n",
      "Epoch 7, iter 290, loss: 8.72381591796875\n",
      "Epoch 7, iter 300, loss: 20.19198989868164\n",
      "Epoch 7, iter 310, loss: 17.01692771911621\n",
      "Epoch 7, iter 320, loss: 41.071327209472656\n",
      "Epoch 7, iter 330, loss: 4.297261714935303\n",
      "Epoch 7, iter 340, loss: 8.989872932434082\n",
      "Epoch 7, iter 350, loss: 6.600051403045654\n",
      "Epoch 7, iter 360, loss: 80.57453155517578\n",
      "Epoch 7, iter 370, loss: 53.07530975341797\n",
      "Epoch 7, iter 380, loss: 13.008549690246582\n",
      "Epoch 7, iter 390, loss: 8.672477722167969\n",
      "Epoch 7, iter 400, loss: 4.437113285064697\n",
      "Epoch 7, iter 410, loss: 3.8138930797576904\n",
      "Epoch 8, iter 0, loss: 71.69352722167969\n",
      "Epoch 8, iter 10, loss: 9.137743949890137\n",
      "Epoch 8, iter 20, loss: 7.726169109344482\n",
      "Epoch 8, iter 30, loss: 6.363821506500244\n",
      "Epoch 8, iter 40, loss: 4.865668773651123\n",
      "Epoch 8, iter 50, loss: 4.918726444244385\n",
      "Epoch 8, iter 60, loss: 9.718289375305176\n",
      "Epoch 8, iter 70, loss: 16.744184494018555\n",
      "Epoch 8, iter 80, loss: 17.899675369262695\n",
      "Epoch 8, iter 90, loss: 15.70594596862793\n",
      "Epoch 8, iter 100, loss: 3.7134721279144287\n",
      "Epoch 8, iter 110, loss: 5.7999162673950195\n",
      "Epoch 8, iter 120, loss: 78.6750717163086\n",
      "Epoch 8, iter 130, loss: 4.852970123291016\n",
      "Epoch 8, iter 140, loss: 8.259671211242676\n",
      "Epoch 8, iter 150, loss: 10.891945838928223\n",
      "Epoch 8, iter 160, loss: 13.58664608001709\n",
      "Epoch 8, iter 170, loss: 11.166044235229492\n",
      "Epoch 8, iter 180, loss: 11.848504066467285\n",
      "Epoch 8, iter 190, loss: 4.493041515350342\n",
      "Epoch 8, iter 200, loss: 6.263633728027344\n",
      "Epoch 8, iter 210, loss: 7.16339111328125\n",
      "Epoch 8, iter 220, loss: 8.944112777709961\n",
      "Epoch 8, iter 230, loss: 5.591317176818848\n",
      "Epoch 8, iter 240, loss: 6.126336574554443\n",
      "Epoch 8, iter 250, loss: 6.186532497406006\n",
      "Epoch 8, iter 260, loss: 4.817872047424316\n",
      "Epoch 8, iter 270, loss: 8.992440223693848\n",
      "Epoch 8, iter 280, loss: 4.065360069274902\n",
      "Epoch 8, iter 290, loss: 2.452897310256958\n",
      "Epoch 8, iter 300, loss: 5.996439456939697\n",
      "Epoch 8, iter 310, loss: 4.336985111236572\n",
      "Epoch 8, iter 320, loss: 60.82472610473633\n",
      "Epoch 8, iter 330, loss: 8.18736743927002\n",
      "Epoch 8, iter 340, loss: 9.332560539245605\n",
      "Epoch 8, iter 350, loss: 6.7380547523498535\n",
      "Epoch 8, iter 360, loss: 3.224320888519287\n",
      "Epoch 8, iter 370, loss: 20.702077865600586\n",
      "Epoch 8, iter 380, loss: 15.951083183288574\n",
      "Epoch 8, iter 390, loss: 28.616933822631836\n",
      "Epoch 8, iter 400, loss: 8.463017463684082\n",
      "Epoch 8, iter 410, loss: 9.71835994720459\n",
      "Epoch 9, iter 0, loss: 6.207650184631348\n",
      "Epoch 9, iter 10, loss: 6.048431873321533\n",
      "Epoch 9, iter 20, loss: 9.85693073272705\n",
      "Epoch 9, iter 30, loss: 9.512099266052246\n",
      "Epoch 9, iter 40, loss: 64.02962493896484\n",
      "Epoch 9, iter 50, loss: 12.387985229492188\n",
      "Epoch 9, iter 60, loss: 8.159865379333496\n",
      "Epoch 9, iter 70, loss: 2.930644989013672\n",
      "Epoch 9, iter 80, loss: 6.75300931930542\n",
      "Epoch 9, iter 90, loss: 5.01753568649292\n",
      "Epoch 9, iter 100, loss: 3.367476224899292\n",
      "Epoch 9, iter 110, loss: 4.537442207336426\n",
      "Epoch 9, iter 120, loss: 5.248259544372559\n",
      "Epoch 9, iter 130, loss: 7.222451686859131\n",
      "Epoch 9, iter 140, loss: 2.5986130237579346\n",
      "Epoch 9, iter 150, loss: 4.9989142417907715\n",
      "Epoch 9, iter 160, loss: 7.476592063903809\n",
      "Epoch 9, iter 170, loss: 2.067457914352417\n",
      "Epoch 9, iter 180, loss: 4.568399429321289\n",
      "Epoch 9, iter 190, loss: 10.129521369934082\n",
      "Epoch 9, iter 200, loss: 5.8185529708862305\n",
      "Epoch 9, iter 210, loss: 4.816145896911621\n",
      "Epoch 9, iter 220, loss: 5.434292793273926\n",
      "Epoch 9, iter 230, loss: 5.105734348297119\n",
      "Epoch 9, iter 240, loss: 48.652076721191406\n",
      "Epoch 9, iter 250, loss: 15.293999671936035\n",
      "Epoch 9, iter 260, loss: 3.8216612339019775\n",
      "Epoch 9, iter 270, loss: 50.6727294921875\n",
      "Epoch 9, iter 280, loss: 16.77288246154785\n",
      "Epoch 9, iter 290, loss: 9.67914867401123\n",
      "Epoch 9, iter 300, loss: 5.281251430511475\n",
      "Epoch 9, iter 310, loss: 6.568497180938721\n",
      "Epoch 9, iter 320, loss: 6.82360315322876\n",
      "Epoch 9, iter 330, loss: 3.360326051712036\n",
      "Epoch 9, iter 340, loss: 4.945076942443848\n",
      "Epoch 9, iter 350, loss: 3.211721181869507\n",
      "Epoch 9, iter 360, loss: 10.120635986328125\n",
      "Epoch 9, iter 370, loss: 13.33197021484375\n",
      "Epoch 9, iter 380, loss: 5.720635890960693\n",
      "Epoch 9, iter 390, loss: 4.129244804382324\n",
      "Epoch 9, iter 400, loss: 4.416668891906738\n",
      "Epoch 9, iter 410, loss: 12.892745971679688\n",
      "Epoch 10, iter 0, loss: 3.200530529022217\n",
      "Epoch 10, iter 10, loss: 4.570461273193359\n",
      "Epoch 10, iter 20, loss: 10.442994117736816\n",
      "Epoch 10, iter 30, loss: 4.020328044891357\n",
      "Epoch 10, iter 40, loss: 2.2124061584472656\n",
      "Epoch 10, iter 50, loss: 4.308340549468994\n",
      "Epoch 10, iter 60, loss: 16.29056167602539\n",
      "Epoch 10, iter 70, loss: 7.200011253356934\n",
      "Epoch 10, iter 80, loss: 8.532954216003418\n",
      "Epoch 10, iter 90, loss: 46.327781677246094\n",
      "Epoch 10, iter 100, loss: 10.432211875915527\n",
      "Epoch 10, iter 110, loss: 11.969511985778809\n",
      "Epoch 10, iter 120, loss: 43.19437789916992\n",
      "Epoch 10, iter 130, loss: 10.825112342834473\n",
      "Epoch 10, iter 140, loss: 17.49563217163086\n",
      "Epoch 10, iter 150, loss: 3.5858917236328125\n",
      "Epoch 10, iter 160, loss: 5.662631034851074\n",
      "Epoch 10, iter 170, loss: 4.45444917678833\n",
      "Epoch 10, iter 180, loss: 8.075242042541504\n",
      "Epoch 10, iter 190, loss: 5.135873317718506\n",
      "Epoch 10, iter 200, loss: 5.564826965332031\n",
      "Epoch 10, iter 210, loss: 3.68703293800354\n",
      "Epoch 10, iter 220, loss: 3.1808834075927734\n",
      "Epoch 10, iter 230, loss: 2.7098255157470703\n",
      "Epoch 10, iter 240, loss: 4.925199031829834\n",
      "Epoch 10, iter 250, loss: 1.9533159732818604\n",
      "Epoch 10, iter 260, loss: 4.415252685546875\n",
      "Epoch 10, iter 270, loss: 8.230690956115723\n",
      "Epoch 10, iter 280, loss: 4.703051567077637\n",
      "Epoch 10, iter 290, loss: 3.6439285278320312\n",
      "Epoch 10, iter 300, loss: 9.85492992401123\n",
      "Epoch 10, iter 310, loss: 1.5101163387298584\n",
      "Epoch 10, iter 320, loss: 41.15655517578125\n",
      "Epoch 10, iter 330, loss: 2.242283582687378\n",
      "Epoch 10, iter 340, loss: 3.526526927947998\n",
      "Epoch 10, iter 350, loss: 5.543754577636719\n",
      "Epoch 10, iter 360, loss: 7.010505676269531\n",
      "Epoch 10, iter 370, loss: 9.659844398498535\n",
      "Epoch 10, iter 380, loss: 5.683274269104004\n",
      "Epoch 10, iter 390, loss: 11.158666610717773\n",
      "Epoch 10, iter 400, loss: 3.8265395164489746\n",
      "Epoch 10, iter 410, loss: 7.15865421295166\n",
      "Epoch 11, iter 0, loss: 7.663056373596191\n",
      "Epoch 11, iter 10, loss: 1.1648873090744019\n",
      "Epoch 11, iter 20, loss: 2.2813150882720947\n",
      "Epoch 11, iter 30, loss: 3.5556232929229736\n",
      "Epoch 11, iter 40, loss: 8.944427490234375\n",
      "Epoch 11, iter 50, loss: 2.939849853515625\n",
      "Epoch 11, iter 60, loss: 2.1680147647857666\n",
      "Epoch 11, iter 70, loss: 1.7671453952789307\n",
      "Epoch 11, iter 80, loss: 2.786494016647339\n",
      "Epoch 11, iter 90, loss: 3.784864902496338\n",
      "Epoch 11, iter 100, loss: 9.226632118225098\n",
      "Epoch 11, iter 110, loss: 2.6495728492736816\n",
      "Epoch 11, iter 120, loss: 55.05980682373047\n",
      "Epoch 11, iter 130, loss: 1.9868172407150269\n",
      "Epoch 11, iter 140, loss: 5.439872741699219\n",
      "Epoch 11, iter 150, loss: 10.537763595581055\n",
      "Epoch 11, iter 160, loss: 2.318575143814087\n",
      "Epoch 11, iter 170, loss: 2.5808827877044678\n",
      "Epoch 11, iter 180, loss: 5.153555870056152\n",
      "Epoch 11, iter 190, loss: 5.816463947296143\n",
      "Epoch 11, iter 200, loss: 3.7955338954925537\n",
      "Epoch 11, iter 210, loss: 8.02369499206543\n",
      "Epoch 11, iter 220, loss: 8.740363121032715\n",
      "Epoch 11, iter 230, loss: 58.615325927734375\n",
      "Epoch 11, iter 240, loss: 49.33663558959961\n",
      "Epoch 11, iter 250, loss: 4.305536270141602\n",
      "Epoch 11, iter 260, loss: 5.10847806930542\n",
      "Epoch 11, iter 270, loss: 3.392040729522705\n",
      "Epoch 11, iter 280, loss: 8.173345565795898\n",
      "Epoch 11, iter 290, loss: 3.722909927368164\n",
      "Epoch 11, iter 300, loss: 1.246497392654419\n",
      "Epoch 11, iter 310, loss: 4.8884758949279785\n",
      "Epoch 11, iter 320, loss: 1.5278911590576172\n",
      "Epoch 11, iter 330, loss: 2.0503411293029785\n",
      "Epoch 11, iter 340, loss: 14.330389976501465\n",
      "Epoch 11, iter 350, loss: 23.95460319519043\n",
      "Epoch 11, iter 360, loss: 8.181258201599121\n",
      "Epoch 11, iter 370, loss: 1.9182970523834229\n",
      "Epoch 11, iter 380, loss: 7.418705940246582\n",
      "Epoch 11, iter 390, loss: 15.789949417114258\n",
      "Epoch 11, iter 400, loss: 31.411710739135742\n",
      "Epoch 11, iter 410, loss: 4.353236198425293\n",
      "Epoch 12, iter 0, loss: 3.9202613830566406\n",
      "Epoch 12, iter 10, loss: 13.081561088562012\n",
      "Epoch 12, iter 20, loss: 30.017627716064453\n",
      "Epoch 12, iter 30, loss: 2.0698401927948\n",
      "Epoch 12, iter 40, loss: 1.3408948183059692\n",
      "Epoch 12, iter 50, loss: 9.27984619140625\n",
      "Epoch 12, iter 60, loss: 4.273536205291748\n",
      "Epoch 12, iter 70, loss: 3.867950677871704\n",
      "Epoch 12, iter 80, loss: 4.804637432098389\n",
      "Epoch 12, iter 90, loss: 5.707713603973389\n",
      "Epoch 12, iter 100, loss: 4.7651777267456055\n",
      "Epoch 12, iter 110, loss: 43.80926513671875\n",
      "Epoch 12, iter 120, loss: 3.2990853786468506\n",
      "Epoch 12, iter 130, loss: 3.1895790100097656\n",
      "Epoch 12, iter 140, loss: 1.9491665363311768\n",
      "Epoch 12, iter 150, loss: 299.2314147949219\n",
      "Epoch 12, iter 160, loss: 5.930563449859619\n",
      "Epoch 12, iter 170, loss: 4.698166370391846\n",
      "Epoch 12, iter 180, loss: 11.109846115112305\n",
      "Epoch 12, iter 190, loss: 7.101302146911621\n",
      "Epoch 12, iter 200, loss: 2.22607684135437\n",
      "Epoch 12, iter 210, loss: 2.7460551261901855\n",
      "Epoch 12, iter 220, loss: 4.488844871520996\n",
      "Epoch 12, iter 230, loss: 4.584124565124512\n",
      "Epoch 12, iter 240, loss: 3.0296342372894287\n",
      "Epoch 12, iter 250, loss: 3.6711277961730957\n",
      "Epoch 12, iter 260, loss: 3.4506514072418213\n",
      "Epoch 12, iter 270, loss: 4.969546794891357\n",
      "Epoch 12, iter 280, loss: 2.975292205810547\n",
      "Epoch 12, iter 290, loss: 2.4692628383636475\n",
      "Epoch 12, iter 300, loss: 1.467861533164978\n",
      "Epoch 12, iter 310, loss: 2.043428421020508\n",
      "Epoch 12, iter 320, loss: 5.1657586097717285\n",
      "Epoch 12, iter 330, loss: 20.313688278198242\n",
      "Epoch 12, iter 340, loss: 7.467766761779785\n",
      "Epoch 12, iter 350, loss: 2.5000665187835693\n",
      "Epoch 12, iter 360, loss: 5.331442356109619\n",
      "Epoch 12, iter 370, loss: 2.875232696533203\n",
      "Epoch 12, iter 380, loss: 1.2923567295074463\n",
      "Epoch 12, iter 390, loss: 10.680946350097656\n",
      "Epoch 12, iter 400, loss: 2.779649019241333\n",
      "Epoch 12, iter 410, loss: 4.133341312408447\n",
      "Epoch 13, iter 0, loss: 11.87131404876709\n",
      "Epoch 13, iter 10, loss: 9.824244499206543\n",
      "Epoch 13, iter 20, loss: 5.344132423400879\n",
      "Epoch 13, iter 30, loss: 3.3837831020355225\n",
      "Epoch 13, iter 40, loss: 35.67548751831055\n",
      "Epoch 13, iter 50, loss: 3.445565938949585\n",
      "Epoch 13, iter 60, loss: 4.119636535644531\n",
      "Epoch 13, iter 70, loss: 3.1542439460754395\n",
      "Epoch 13, iter 80, loss: 5.229659080505371\n",
      "Epoch 13, iter 90, loss: 2.3081037998199463\n",
      "Epoch 13, iter 100, loss: 43.05997848510742\n",
      "Epoch 13, iter 110, loss: 52.355918884277344\n",
      "Epoch 13, iter 120, loss: 1.487984299659729\n",
      "Epoch 13, iter 130, loss: 4.241612911224365\n",
      "Epoch 13, iter 140, loss: 292.7635803222656\n",
      "Epoch 13, iter 150, loss: 12.430771827697754\n",
      "Epoch 13, iter 160, loss: 4.365018367767334\n",
      "Epoch 13, iter 170, loss: 9.52415943145752\n",
      "Epoch 13, iter 180, loss: 7.832954406738281\n",
      "Epoch 13, iter 190, loss: 3.580782413482666\n",
      "Epoch 13, iter 200, loss: 2.662424087524414\n",
      "Epoch 13, iter 210, loss: 2.4295589923858643\n",
      "Epoch 13, iter 220, loss: 5.2596611976623535\n",
      "Epoch 13, iter 230, loss: 12.984970092773438\n",
      "Epoch 13, iter 240, loss: 5.910775661468506\n",
      "Epoch 13, iter 250, loss: 16.8736515045166\n",
      "Epoch 13, iter 260, loss: 3.3429019451141357\n",
      "Epoch 13, iter 270, loss: 4.226202487945557\n",
      "Epoch 13, iter 280, loss: 4.301699638366699\n",
      "Epoch 13, iter 290, loss: 6.0098557472229\n",
      "Epoch 13, iter 300, loss: 5.806151390075684\n",
      "Epoch 13, iter 310, loss: 1.407317876815796\n",
      "Epoch 13, iter 320, loss: 1.268236756324768\n",
      "Epoch 13, iter 330, loss: 5.934699535369873\n",
      "Epoch 13, iter 340, loss: 3.1403896808624268\n",
      "Epoch 13, iter 350, loss: 1.7303847074508667\n",
      "Epoch 13, iter 360, loss: 9.098697662353516\n",
      "Epoch 13, iter 370, loss: 2.1534974575042725\n",
      "Epoch 13, iter 380, loss: 2.428180694580078\n",
      "Epoch 13, iter 390, loss: 6.22088098526001\n",
      "Epoch 13, iter 400, loss: 11.518333435058594\n",
      "Epoch 13, iter 410, loss: 27.71346092224121\n",
      "Epoch 14, iter 0, loss: 1.9555591344833374\n",
      "Epoch 14, iter 10, loss: 4.082674503326416\n",
      "Epoch 14, iter 20, loss: 3.620572566986084\n",
      "Epoch 14, iter 30, loss: 45.843963623046875\n",
      "Epoch 14, iter 40, loss: 5.277140140533447\n",
      "Epoch 14, iter 50, loss: 6.041456699371338\n",
      "Epoch 14, iter 60, loss: 1.7878726720809937\n",
      "Epoch 14, iter 70, loss: 1.6526681184768677\n",
      "Epoch 14, iter 80, loss: 2.0343329906463623\n",
      "Epoch 14, iter 90, loss: 1.5184146165847778\n",
      "Epoch 14, iter 100, loss: 1.1581370830535889\n",
      "Epoch 14, iter 110, loss: 3.6080212593078613\n",
      "Epoch 14, iter 120, loss: 3.353917360305786\n",
      "Epoch 14, iter 130, loss: 1.3545464277267456\n",
      "Epoch 14, iter 140, loss: 2.3949177265167236\n",
      "Epoch 14, iter 150, loss: 2.6373066902160645\n",
      "Epoch 14, iter 160, loss: 4.262478828430176\n",
      "Epoch 14, iter 170, loss: 3.7044334411621094\n",
      "Epoch 14, iter 180, loss: 3.0688328742980957\n",
      "Epoch 14, iter 190, loss: 1.3013352155685425\n",
      "Epoch 14, iter 200, loss: 12.171290397644043\n",
      "Epoch 14, iter 210, loss: 7.886448860168457\n",
      "Epoch 14, iter 220, loss: 1.6409825086593628\n",
      "Epoch 14, iter 230, loss: 2.3090732097625732\n",
      "Epoch 14, iter 240, loss: 8.042823791503906\n",
      "Epoch 14, iter 250, loss: 2.7365071773529053\n",
      "Epoch 14, iter 260, loss: 3.569619655609131\n",
      "Epoch 14, iter 270, loss: 2.9126346111297607\n",
      "Epoch 14, iter 280, loss: 2.981424331665039\n",
      "Epoch 14, iter 290, loss: 11.16532039642334\n",
      "Epoch 14, iter 300, loss: 2.734057903289795\n",
      "Epoch 14, iter 310, loss: 1.5520554780960083\n",
      "Epoch 14, iter 320, loss: 1.4749687910079956\n",
      "Epoch 14, iter 330, loss: 2.5814356803894043\n",
      "Epoch 14, iter 340, loss: 3.676617383956909\n",
      "Epoch 14, iter 350, loss: 4.038573741912842\n",
      "Epoch 14, iter 360, loss: 1.8794000148773193\n",
      "Epoch 14, iter 370, loss: 3.5950264930725098\n",
      "Epoch 14, iter 380, loss: 1.6815884113311768\n",
      "Epoch 14, iter 390, loss: 7.505734920501709\n",
      "Epoch 14, iter 400, loss: 3.668165683746338\n",
      "Epoch 14, iter 410, loss: 1.9133546352386475\n",
      "Epoch 15, iter 0, loss: 1.639837622642517\n",
      "Epoch 15, iter 10, loss: 1.1404026746749878\n",
      "Epoch 15, iter 20, loss: 4.532746315002441\n",
      "Epoch 15, iter 30, loss: 47.03957748413086\n",
      "Epoch 15, iter 40, loss: 1.6241637468338013\n",
      "Epoch 15, iter 50, loss: 1.4348593950271606\n",
      "Epoch 15, iter 60, loss: 1.4170072078704834\n",
      "Epoch 15, iter 70, loss: 1.912046194076538\n",
      "Epoch 15, iter 80, loss: 4.238410949707031\n",
      "Epoch 15, iter 90, loss: 3.514390468597412\n",
      "Epoch 15, iter 100, loss: 17.792680740356445\n",
      "Epoch 15, iter 110, loss: 1.2187299728393555\n",
      "Epoch 15, iter 120, loss: 2.9681732654571533\n",
      "Epoch 15, iter 130, loss: 3.579508066177368\n",
      "Epoch 15, iter 140, loss: 7.08886194229126\n",
      "Epoch 15, iter 150, loss: 1.9805402755737305\n",
      "Epoch 15, iter 160, loss: 1.2038342952728271\n",
      "Epoch 15, iter 170, loss: 1.2769588232040405\n",
      "Epoch 15, iter 180, loss: 1.9564456939697266\n",
      "Epoch 15, iter 190, loss: 3.2500953674316406\n",
      "Epoch 15, iter 200, loss: 3.8610284328460693\n",
      "Epoch 15, iter 210, loss: 4.364454746246338\n",
      "Epoch 15, iter 220, loss: 5.312808513641357\n",
      "Epoch 15, iter 230, loss: 3.194000720977783\n",
      "Epoch 15, iter 240, loss: 5.006213188171387\n",
      "Epoch 15, iter 250, loss: 3.7876641750335693\n",
      "Epoch 15, iter 260, loss: 2.3589046001434326\n",
      "Epoch 15, iter 270, loss: 5.612343788146973\n",
      "Epoch 15, iter 280, loss: 6.384801387786865\n",
      "Epoch 15, iter 290, loss: 3.730152130126953\n",
      "Epoch 15, iter 300, loss: 3.2247731685638428\n",
      "Epoch 15, iter 310, loss: 8.050057411193848\n",
      "Epoch 15, iter 320, loss: 4.225649356842041\n",
      "Epoch 15, iter 330, loss: 2.691800832748413\n",
      "Epoch 15, iter 340, loss: 3.0772054195404053\n",
      "Epoch 15, iter 350, loss: 73.90728759765625\n",
      "Epoch 15, iter 360, loss: 3.0655155181884766\n",
      "Epoch 15, iter 370, loss: 40.682472229003906\n",
      "Epoch 15, iter 380, loss: 1.6607521772384644\n",
      "Epoch 15, iter 390, loss: 1.1104077100753784\n",
      "Epoch 15, iter 400, loss: 1.7353975772857666\n",
      "Epoch 15, iter 410, loss: 7.4785847663879395\n",
      "Epoch 16, iter 0, loss: 5.362365245819092\n",
      "Epoch 16, iter 10, loss: 1.4790642261505127\n",
      "Epoch 16, iter 20, loss: 1.875985860824585\n",
      "Epoch 16, iter 30, loss: 0.7762742638587952\n",
      "Epoch 16, iter 40, loss: 1.962950348854065\n",
      "Epoch 16, iter 50, loss: 1.2192171812057495\n",
      "Epoch 16, iter 60, loss: 2.8897814750671387\n",
      "Epoch 16, iter 70, loss: 2.8216469287872314\n",
      "Epoch 16, iter 80, loss: 0.9567959904670715\n",
      "Epoch 16, iter 90, loss: 3.8927173614501953\n",
      "Epoch 16, iter 100, loss: 0.6778860688209534\n",
      "Epoch 16, iter 110, loss: 6.736621856689453\n",
      "Epoch 16, iter 120, loss: 5.36476469039917\n",
      "Epoch 16, iter 130, loss: 4.541292190551758\n",
      "Epoch 16, iter 140, loss: 8.337145805358887\n",
      "Epoch 16, iter 150, loss: 5.50106954574585\n",
      "Epoch 16, iter 160, loss: 3.6479763984680176\n",
      "Epoch 16, iter 170, loss: 16.422313690185547\n",
      "Epoch 16, iter 180, loss: 7.115399360656738\n",
      "Epoch 16, iter 190, loss: 8.676440238952637\n",
      "Epoch 16, iter 200, loss: 43.65195846557617\n",
      "Epoch 16, iter 210, loss: 1.96527099609375\n",
      "Epoch 16, iter 220, loss: 3.8633103370666504\n",
      "Epoch 16, iter 230, loss: 4.314237117767334\n",
      "Epoch 16, iter 240, loss: 1.6086304187774658\n",
      "Epoch 16, iter 250, loss: 1.6772263050079346\n",
      "Epoch 16, iter 260, loss: 4.851244926452637\n",
      "Epoch 16, iter 270, loss: 3.78021502494812\n",
      "Epoch 16, iter 280, loss: 2.04303240776062\n",
      "Epoch 16, iter 290, loss: 2.1406729221343994\n",
      "Epoch 16, iter 300, loss: 1.604941487312317\n",
      "Epoch 16, iter 310, loss: 6.34601354598999\n",
      "Epoch 16, iter 320, loss: 1.9877300262451172\n",
      "Epoch 16, iter 330, loss: 2.2722067832946777\n",
      "Epoch 16, iter 340, loss: 51.366294860839844\n",
      "Epoch 16, iter 350, loss: 1.8721262216567993\n",
      "Epoch 16, iter 360, loss: 1.5897693634033203\n",
      "Epoch 16, iter 370, loss: 2.6986279487609863\n",
      "Epoch 16, iter 380, loss: 4.478030204772949\n",
      "Epoch 16, iter 390, loss: 1.6631946563720703\n",
      "Epoch 16, iter 400, loss: 2.536005735397339\n",
      "Epoch 16, iter 410, loss: 4.8097076416015625\n",
      "Epoch 17, iter 0, loss: 3.0032646656036377\n",
      "Epoch 17, iter 10, loss: 3.3509652614593506\n",
      "Epoch 17, iter 20, loss: 1.7191661596298218\n",
      "Epoch 17, iter 30, loss: 2.246755361557007\n",
      "Epoch 17, iter 40, loss: 3.8236024379730225\n",
      "Epoch 17, iter 50, loss: 4.195671558380127\n",
      "Epoch 17, iter 60, loss: 5.622458457946777\n",
      "Epoch 17, iter 70, loss: 3.740642786026001\n",
      "Epoch 17, iter 80, loss: 2.4923133850097656\n",
      "Epoch 17, iter 90, loss: 4.925055503845215\n",
      "Epoch 17, iter 100, loss: 3.4462430477142334\n",
      "Epoch 17, iter 110, loss: 3.639647960662842\n",
      "Epoch 17, iter 120, loss: 1.508996844291687\n",
      "Epoch 17, iter 130, loss: 1.1598745584487915\n",
      "Epoch 17, iter 140, loss: 2.0490403175354004\n",
      "Epoch 17, iter 150, loss: 2.1357262134552\n",
      "Epoch 17, iter 160, loss: 3.084430456161499\n",
      "Epoch 17, iter 170, loss: 1.1446622610092163\n",
      "Epoch 17, iter 180, loss: 8.863749504089355\n",
      "Epoch 17, iter 190, loss: 3.319728136062622\n",
      "Epoch 17, iter 200, loss: 0.4198625683784485\n",
      "Epoch 17, iter 210, loss: 3.1399054527282715\n",
      "Epoch 17, iter 220, loss: 1.1798746585845947\n",
      "Epoch 17, iter 230, loss: 1.3667887449264526\n",
      "Epoch 17, iter 240, loss: 2.9154982566833496\n",
      "Epoch 17, iter 250, loss: 4.02889347076416\n",
      "Epoch 17, iter 260, loss: 3.3902409076690674\n",
      "Epoch 17, iter 270, loss: 3.2829089164733887\n",
      "Epoch 17, iter 280, loss: 3.209120035171509\n",
      "Epoch 17, iter 290, loss: 1.8337734937667847\n",
      "Epoch 17, iter 300, loss: 1.324670672416687\n",
      "Epoch 17, iter 310, loss: 2.051591396331787\n",
      "Epoch 17, iter 320, loss: 2.1199629306793213\n",
      "Epoch 17, iter 330, loss: 0.729089617729187\n",
      "Epoch 17, iter 340, loss: 0.9696937203407288\n",
      "Epoch 17, iter 350, loss: 5.149816036224365\n",
      "Epoch 17, iter 360, loss: 2.9575581550598145\n",
      "Epoch 17, iter 370, loss: 1.7289068698883057\n",
      "Epoch 17, iter 380, loss: 7.422911167144775\n",
      "Epoch 17, iter 390, loss: 2.233010768890381\n",
      "Epoch 17, iter 400, loss: 10.380922317504883\n",
      "Epoch 17, iter 410, loss: 3.165241241455078\n",
      "Epoch 18, iter 0, loss: 3.3361704349517822\n",
      "Epoch 18, iter 10, loss: 1.885594367980957\n",
      "Epoch 18, iter 20, loss: 2.970337390899658\n",
      "Epoch 18, iter 30, loss: 1.2487698793411255\n",
      "Epoch 18, iter 40, loss: 1.7521592378616333\n",
      "Epoch 18, iter 50, loss: 1.449690818786621\n",
      "Epoch 18, iter 60, loss: 3.65696382522583\n",
      "Epoch 18, iter 70, loss: 2.822355031967163\n",
      "Epoch 18, iter 80, loss: 5.482501029968262\n",
      "Epoch 18, iter 90, loss: 1.4869396686553955\n",
      "Epoch 18, iter 100, loss: 3.7235357761383057\n",
      "Epoch 18, iter 110, loss: 4.0884504318237305\n",
      "Epoch 18, iter 120, loss: 3.0830891132354736\n",
      "Epoch 18, iter 130, loss: 1.6308902502059937\n",
      "Epoch 18, iter 140, loss: 1.003265142440796\n",
      "Epoch 18, iter 150, loss: 2.4906461238861084\n",
      "Epoch 18, iter 160, loss: 4.34985876083374\n",
      "Epoch 18, iter 170, loss: 3.7315919399261475\n",
      "Epoch 18, iter 180, loss: 1.3276396989822388\n",
      "Epoch 18, iter 190, loss: 2.762671947479248\n",
      "Epoch 18, iter 200, loss: 1.197021245956421\n",
      "Epoch 18, iter 210, loss: 1.3393803834915161\n",
      "Epoch 18, iter 220, loss: 2.315397262573242\n",
      "Epoch 18, iter 230, loss: 3.026960611343384\n",
      "Epoch 18, iter 240, loss: 3.512467622756958\n",
      "Epoch 18, iter 250, loss: 2.9492735862731934\n",
      "Epoch 18, iter 260, loss: 2.269231081008911\n",
      "Epoch 18, iter 270, loss: 5.860838413238525\n",
      "Epoch 18, iter 280, loss: 1.608356237411499\n",
      "Epoch 18, iter 290, loss: 4.821961402893066\n",
      "Epoch 18, iter 300, loss: 3.491941213607788\n",
      "Epoch 18, iter 310, loss: 1.4116178750991821\n",
      "Epoch 18, iter 320, loss: 4.705935478210449\n",
      "Epoch 18, iter 330, loss: 2.1660759449005127\n",
      "Epoch 18, iter 340, loss: 3.103564977645874\n",
      "Epoch 18, iter 350, loss: 1.0912429094314575\n",
      "Epoch 18, iter 360, loss: 4.577449321746826\n",
      "Epoch 18, iter 370, loss: 47.53523635864258\n",
      "Epoch 18, iter 380, loss: 2.1538360118865967\n",
      "Epoch 18, iter 390, loss: 1.1061688661575317\n",
      "Epoch 18, iter 400, loss: 0.6883702278137207\n",
      "Epoch 18, iter 410, loss: 9.458564758300781\n",
      "Epoch 19, iter 0, loss: 0.6773232817649841\n",
      "Epoch 19, iter 10, loss: 1.290338397026062\n",
      "Epoch 19, iter 20, loss: 4.7953081130981445\n",
      "Epoch 19, iter 30, loss: 22.090238571166992\n",
      "Epoch 19, iter 40, loss: 5.1451802253723145\n",
      "Epoch 19, iter 50, loss: 3.111603260040283\n",
      "Epoch 19, iter 60, loss: 2.2752456665039062\n",
      "Epoch 19, iter 70, loss: 1.651557207107544\n",
      "Epoch 19, iter 80, loss: 1.916586995124817\n",
      "Epoch 19, iter 90, loss: 1.2464009523391724\n",
      "Epoch 19, iter 100, loss: 1.673362374305725\n",
      "Epoch 19, iter 110, loss: 2.876904249191284\n",
      "Epoch 19, iter 120, loss: 3.475527048110962\n",
      "Epoch 19, iter 130, loss: 1.5167579650878906\n",
      "Epoch 19, iter 140, loss: 2.1881697177886963\n",
      "Epoch 19, iter 150, loss: 0.3350583612918854\n",
      "Epoch 19, iter 160, loss: 1.1148163080215454\n",
      "Epoch 19, iter 170, loss: 5.029771327972412\n",
      "Epoch 19, iter 180, loss: 9.480840682983398\n",
      "Epoch 19, iter 190, loss: 1.236535668373108\n",
      "Epoch 19, iter 200, loss: 0.6159273386001587\n",
      "Epoch 19, iter 210, loss: 1.5384387969970703\n",
      "Epoch 19, iter 220, loss: 3.1589725017547607\n",
      "Epoch 19, iter 230, loss: 1.2430763244628906\n",
      "Epoch 19, iter 240, loss: 3.5491223335266113\n",
      "Epoch 19, iter 250, loss: 0.98565274477005\n",
      "Epoch 19, iter 260, loss: 1.9159777164459229\n",
      "Epoch 19, iter 270, loss: 4.416323184967041\n",
      "Epoch 19, iter 280, loss: 5.230174541473389\n",
      "Epoch 19, iter 290, loss: 1.100682020187378\n",
      "Epoch 19, iter 300, loss: 3.4177823066711426\n",
      "Epoch 19, iter 310, loss: 0.7221174836158752\n",
      "Epoch 19, iter 320, loss: 1.8301985263824463\n",
      "Epoch 19, iter 330, loss: 1.841476321220398\n",
      "Epoch 19, iter 340, loss: 0.5789139866828918\n",
      "Epoch 19, iter 350, loss: 2.3592212200164795\n",
      "Epoch 19, iter 360, loss: 1.9021177291870117\n",
      "Epoch 19, iter 370, loss: 5.1793437004089355\n",
      "Epoch 19, iter 380, loss: 7.547222137451172\n",
      "Epoch 19, iter 390, loss: 0.8262878060340881\n",
      "Epoch 19, iter 400, loss: 4.071146488189697\n",
      "Epoch 19, iter 410, loss: 1.6460260152816772\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, kps) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        kps = kps.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, kps)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, iter {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"keypoints_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis_analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
